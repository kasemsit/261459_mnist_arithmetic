# MNIST Arithmetic Assignment

Build neural networks that can perform arithmetic operations (addition and multiplication) on MNIST digit images.

In this assignment, there are ğŸ¯ Two Architectures: **Architecture 1 (End-to-end CNN)** and  **Architecture 2 (Multi-task CNN)**.

## Policy

- **âœ… This is an open-book assignment.**
- **âœ… Feel free to discuss concepts and approaches with classmates.**
- **âœ… Each student must submit their own implementation and report.**
- **âœ… You may use online resources, documentation, and tutorials.**
- **âœ… Generative AI such as ChatGPT are allowed.**
- **âŒ Do not copy code directly from others or share complete solutions.**

## ğŸ“ Learning Objectives

- Implement CNN architectures in PyTorch
- Understand training loops and validation
- Practice model evaluation and analysis

## ğŸ—ï¸ Architecture Diagrams

### **Architecture 1: End-to-End CNN**
```
Input: Combined Image (28x56)     Operation Info
        [digit1][digit2]              [op_code]
               â”‚                         â”‚
               â–¼                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
        â”‚   Conv2D    â”‚                  â”‚
        â”‚  (1â†’32â†’64â†’  â”‚                  â”‚
        â”‚   â†’128)     â”‚                  â”‚
        â”‚             â”‚                  â”‚
        â”‚  MaxPool    â”‚                  â”‚
        â”‚  Dropout    â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
               â”‚                         â”‚
               â–¼                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
        â”‚   Flatten   â”‚                  â”‚
        â”‚     FC      â”‚                  â”‚
        â”‚ (2688â†’512â†’  â”‚                  â”‚
        â”‚   â†’256)     â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
               â”‚                         â”‚
               â–¼                         â”‚
        [Image Features: 256]            â”‚
               â”‚                         â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   Concatenate   â”‚
               â”‚ [256 + 1 = 257] â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   Classifier    â”‚
               â”‚   (257â†’91)      â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              [Arithmetic Result: 0-90]

**Loss Function**: CrossEntropyLoss
â€¢ Single-task learning: Direct mapping from image to arithmetic result
â€¢ 91 output classes (0-90 for addition/multiplication results)
â€¢ End-to-end optimization of entire pipeline
**Model** has 1,624,246 parameters
```

### **Architecture 2: Multi-Task CNN**
```
Input: Combined Image (28x56)
        [digit1][digit2]
               â”‚
               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    Split    â”‚
        â”‚   28x56 â†’   â”‚
        â”‚ 2Ã—(28x28)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
        â”‚             â”‚
        â–¼             â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Shared CNN  â”‚ â”‚ Shared CNN  â”‚
 â”‚   (28x28)   â”‚ â”‚   (28x28)   â”‚
 â”‚    â”‚ â”‚ â”‚    â”‚ â”‚    â”‚ â”‚ â”‚    â”‚
 â”‚ Convâ†’Pool   â”‚ â”‚ Convâ†’Pool   â”‚
 â”‚ â†’BatchNorm  â”‚ â”‚ â†’BatchNorm  â”‚
 â”‚    FC       â”‚ â”‚    FC       â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚
        â–¼             â–¼
   [Digit1: 10]  [Digit2: 10]     Operation Info
        â”‚             â”‚              [op_code]
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                  â”‚
               â”‚                         â”‚
               â–¼                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
        â”‚   Concatenate   â”‚             â”‚
        â”‚ [10 + 10 + 1]   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚    = 21         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Operation MLP   â”‚
        â”‚  (21â†’128â†’64)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Classifier    â”‚
        â”‚    (64â†’91)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
      [Arithmetic Result: 0-90]

**Loss Function**: Multi-task Weighted Loss
â€¢ Combined loss: `result_loss + 0.3 * (digit1_loss + digit2_loss)`
â€¢ Three CrossEntropyLoss functions for each output
â€¢ Multi-task learning with explicit digit recognition

**Multi-task Outputs**:
â€¢ Digit 1 prediction (10 classes)
â€¢ Digit 2 prediction (10 classes)
â€¢ Arithmetic result (91 classes)

**Model** has 407,845 parameters
```

## ğŸš€ Quick Start

1. **Create and activate virtual environment**:
   ```bash
   # Create virtual environment
   python -m venv mnist_env

   # Activate virtual environment
   # On Linux/Mac:
   source mnist_env/bin/activate
   # On Windows:
   # mnist_env\Scripts\activate
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Test the dataset**:
   ```bash
   python dataset.py
   ```

4. **Run training (after implementing templates)**:
   ```bash
   # For Architecture 1
   python main1.py

   # For Architecture 2
   python main2.py
   ```


## ğŸ“ Files Overview

### **Provided (Ready to Use)**
- `dataset.py` - Complete MNIST arithmetic dataset with data augmentation support and reproducibility
- `requirements.txt` - All dependencies (works for both assignments)
- `main1.py` - Ready-to-run training script for Architecture 1
- `main2.py` - Ready-to-run training script for Architecture 2


### **Templates (Copy and Complete)**

**Architecture 1:**
- `model_architecture1_template.py` â†’ copy to `model1.py`
- `trainer_architecture1_template.py` â†’ copy to `trainer1.py`
- Run `main1.py` when completed (will run Architecture 1 only)

**Architecture 2:**
- `model_architecture2_template.py` â†’ copy to `model2.py`
- `trainer_architecture2_template.py` â†’ copy to `trainer2.py`
- Run `main2.py` when completed (will run Architecture 2 only)


## ğŸ“ What to do?

1. Complete the **TODO** comments in each template file (4 files totally).
2. Train the models by `python main1.py` and `python main2.py`.
3. You can check you model implementation using `python model1.py` and `python model2.py`.
4. Success criteria âœ… is that the models train without errors and achieve >90% validation accuracy.
   
## ğŸ“ What to submit?

1. Submit `model1.py`, `model2.py`, `trainer1.py`, and `trainer2.py`. 
2. Written report `report.pdf` that
   - shows the steps to compute **the number of parameters** for each architecture **in details** (i.e. how 1,624,246 and 407,845 parameters are derived).
   - **compares** and **discuss** about the two architectures in terms of, for example, model performance, memory efficiency, loss functions, interpretability, etc. 



Good luck! ğŸš€

---